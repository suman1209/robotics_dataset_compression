{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys  \n",
    "sys.path.insert(1, '../')\n",
    "# from utils.dataset_utils import OriginalDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(torch.cuda. get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.dataset_utils import read_img, get_storage\n",
    "from torchvision.datasets import VisionDataset\n",
    "import numpy as np\n",
    "\n",
    "def norm_value(s):\n",
    "    # return (s + 255.0) / 510.0\n",
    "    return s / 255\n",
    "\n",
    "def denorm_value(s):\n",
    "    # return (s * 510.0) - 255.0\n",
    "    # print(f\"{s = }\")\n",
    "    return torch.tensor(s * 255, dtype=torch.int8)\n",
    "\n",
    "class OriginalDataset(VisionDataset):\n",
    "    def __init__(self, data_path: str, color: bool=True):\n",
    "        super(OriginalDataset, self).__init__(self)\n",
    "        self.data_path = data_path\n",
    "        self.color = color\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        this dataset returns the image corresponding to the index\n",
    "        \"\"\"\n",
    "\n",
    "        if idx >= len(self) or idx < 0:\n",
    "            # needed for iterator stop condition\n",
    "            raise IndexError\n",
    "        # the img_file_path existence check\n",
    "        img_path = f'{self.data_path}/idx_{idx}.png'\n",
    "        img2_path = f'{self.data_path}/idx_{idx+1}.png'\n",
    "        assert os.path.exists(img_path), f\"Invalid img_path: {img_path} in {self.data_path}\"\n",
    "        img1 = read_img(img_path, self.color)\n",
    "        # img2 = read_img(img2_path, self.color)\n",
    "        img1 = np.array(img1, dtype = np.float32)\n",
    "        # img2 = np.array(img2, dtype = np.float32)\n",
    "        # return norm_value(img2 - img1)\n",
    "        return norm_value(img1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        dirpath, dir_names, files = next(os.walk(self.data_path))\n",
    "        # return len([i for i in files if \"resize\" not in i]) - 1\n",
    "        return len([i for i in files if \"resize\" not in i])\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"OriginalDataset({self.data_path})\"\n",
    "\n",
    "    def get_storage_size(self, num_images):\n",
    "        \"returns the total storage size of the dataset\"\n",
    "        total_storage = 0\n",
    "        for data in [self[i] for i in range(num_images)]:\n",
    "            total_storage += get_storage(data)\n",
    "        return total_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample_feature = torch.randn(1, 3, 37, 72)\n",
    "# next_vector_predictor = NextVectorPredictor()\n",
    "# pred = next_vector_predictor(sample_feature)\n",
    "# print(f\"{pred.shape = }\")\n",
    "# torch.save(next_vector_predictor.state_dict(), \"./transformer_encoder.pt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "original_dataset = OriginalDataset('../datasets/droid_100_sample_pictures')\n",
    "len_ = (original_dataset.__len__())\n",
    "print(len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 320, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/suman97/utn-msc/f5eea314a0c8498398d70e95568ea7bd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from autoencoder_predictor_combined import CNNAutoencoder, OriginalDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/200], Loss: 0.096654\n",
      "Epoch [ 2/200], Loss: 0.039727\n",
      "Epoch [ 3/200], Loss: 0.030326\n",
      "Epoch [ 4/200], Loss: 0.025328\n",
      "Epoch [ 5/200], Loss: 0.022478\n",
      "Epoch [ 6/200], Loss: 0.019687\n",
      "Epoch [ 7/200], Loss: 0.017365\n",
      "Epoch [ 8/200], Loss: 0.015677\n",
      "Epoch [ 9/200], Loss: 0.014368\n",
      "Epoch [10/200], Loss: 0.013124\n",
      "Epoch [11/200], Loss: 0.012379\n",
      "Epoch [12/200], Loss: 0.011810\n",
      "Epoch [13/200], Loss: 0.011329\n",
      "Epoch [14/200], Loss: 0.010694\n"
     ]
    }
   ],
   "source": [
    "model = CNNAutoencoder()\n",
    "model.fit(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_next_model_predictor()\n",
    "model.save_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model(checkpoint=\"../checkpoints/lowest_loss_ae.pt\")\n",
    "train_loader = torch.utils.data.DataLoader(original_dataset, batch_size=4, shuffle=False)\n",
    "test_image = next(iter(train_loader))[:1]\n",
    "print(f\"{test_image.shape = }\")\n",
    "with torch.no_grad():\n",
    "    feature, next_feature, reconstructed = model.test(test_image.permute(0, 3, 1, 2))\n",
    "    reconstructed = reconstructed.permute(0, 2, 3, 1)\n",
    "print(f\"{reconstructed.shape = }\")\n",
    "print(denorm_value(test_image[0][0][0]))\n",
    "print(denorm_value(reconstructed[0][0][0]))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n",
    "axes[0].imshow(denorm_value(test_image.squeeze()))\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(denorm_value(reconstructed.squeeze()))\n",
    "axes[1].axis('off')\n",
    "print(f\"{feature.shape= }, {next_feature.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import NextVectorPredictor\n",
    "next_vector_predictor = NextVectorPredictor()\n",
    "next_vector_predictor.load_state_dict(torch.load(\"./next_vector_predictor.pt\"))\n",
    "decoder = torch.load(\"./decoder.pt\")\n",
    "print(next_vector_predictor)\n",
    "\n",
    "next_feature = feature\n",
    "features = [next_feature]\n",
    "for i in range(len(original_dataset) - 1):\n",
    "    next_feature = next_vector_predictor(next_feature)\n",
    "    features.append(next_feature)\n",
    "features = torch.stack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from comet_ml import Experiment\n",
    "with open(\"/var/lit2425/jenga/suman/UTN/AIR/semester3/common/common_utils/comet_config.json\", \"r\") as f:\n",
    "    comet_config = json.load(f)\n",
    "if not comet_config[\"local_debug\"]:\n",
    "    exp = Experiment(**comet_config.get(\"comet_cfg\"))\n",
    "    exp.set_name(comet_config[\"experiment_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "######\n",
    "decoded_imgs = [decoder(feat) for feat in features]\n",
    "decoded_imgs = [img.squeeze(dim=0) for img in decoded_imgs][:]\n",
    "decoded_imgs = torch.stack(decoded_imgs)\n",
    "print(decoded_imgs.shape)\n",
    "grid = make_grid(decoded_imgs)\n",
    "\n",
    "img_grid = grid\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "print(f\"{img_grid.shape = }\")\n",
    "import json\n",
    "\n",
    "# try:\n",
    "# orig_images = torch.stack([original_dataset[i] for i in range(len(original_dataset)))\n",
    "# grid = make_grid(orig_images)\n",
    "name = f\"next vector prediction images\"\n",
    "exp.log_image(img_grid, name=name)\n",
    "print(f\"{img_grid.shape = }\")\n",
    "img_grid = img_grid.permute(2, 0, 1)\n",
    "show(img_grid)\n",
    "# except Exception as e:\n",
    "#     print(\"The comet experiment could not be logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @todo Billy can you do this?\n",
    "we need to encode all the 166 images with the encoder, save the encoded vectors get the residual and write a function that will give us the total size of the models and the encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/jenga/suman/mmfm/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNNAutoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([8, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 7, 7]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([8, 16, 7, 7]) from checkpoint, the shape in current model is torch.Size([3, 32, 7, 7]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([3]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([8, 16, 7, 7]) from checkpoint, the shape in current model is torch.Size([3, 32, 7, 7]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 7, 7]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([8, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../checkpoints/lowest_loss_ae.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model = load_model(checkpoint=\"../checkpoints/lowest_loss_ae.pt\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m original_dataset \u001b[38;5;241m=\u001b[39m OriginalDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/droid_100_sample_pictures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/robotics_dataset_compression/research/autoencoder.py:25\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(checkpoint, train)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(checkpoint), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     model \u001b[38;5;241m=\u001b[39m CNNAutoencoder()\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/mmfm/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNNAutoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([8, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 7, 7]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([8, 16, 7, 7]) from checkpoint, the shape in current model is torch.Size([3, 32, 7, 7]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([3]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([8, 16, 7, 7]) from checkpoint, the shape in current model is torch.Size([3, 32, 7, 7]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 7, 7]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([8, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3])."
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '../')\n",
    "from autoencoder import load_model, denorm_value, norm_value\n",
    "from utils.dataset_utils import OriginalDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = load_model(checkpoint=\"../checkpoints/lowest_loss_ae.pt\")\n",
    "# model = load_model(checkpoint=\"../checkpoints/lowest_loss_ae.pt\")\n",
    "original_dataset = OriginalDataset('../datasets/droid_100_sample_pictures')\n",
    "len_ = (original_dataset.__len__())\n",
    "print(\"start\")\n",
    "all_images = []\n",
    "for i in range(len_):\n",
    "    img_org = original_dataset[i]\n",
    "    all_images.append(img_org)\n",
    "print(len(all_images))\n",
    "all_images = torch.tensor(all_images, dtype=torch.float)\n",
    "print(all_images.shape)\n",
    "all_images = norm_value(all_images)\n",
    "with torch.no_grad():\n",
    "    feature, next_feature, reconstructed = model.test(all_images.permute(0, 3, 1, 2))\n",
    "    reconstructed = reconstructed.permute(0, 2, 3, 1)\n",
    "print(feature.shape)\n",
    "# print(img_org[0][0])\n",
    "print(denorm_value(all_images[0][0][0]))\n",
    "print(denorm_value(reconstructed[0][0][0]))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n",
    "axes[0].imshow(denorm_value(all_images[0].squeeze()))\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(denorm_value(reconstructed[0].squeeze()))\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, h, w, c = feature.shape\n",
    "feature_pixel_channel = i * h * w * c\n",
    "print(f\"data size: {feature_pixel_channel * 4 / 1024 / 1024} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = np.array(denorm_value(reconstructed), dtype=np.int16)\n",
    "all_images = np.array(denorm_value(all_images), dtype=np.int16)\n",
    "diff = all_images - reconstructed\n",
    "i, h, w, c = diff.shape\n",
    "pixel_channel = i * h * w * c\n",
    "zero_cnt = np.sum(diff == 0)\n",
    "threshold = 8\n",
    "threshold_cnt = np.sum(np.abs(diff) <= threshold)\n",
    "print(f\"total number of pixels: {pixel_channel}\")\n",
    "print(f\"zero pixels: {zero_cnt}\")\n",
    "print(f\"zero pixels ratio: {zero_cnt / pixel_channel}\")\n",
    "print(f\"threshold pixels: {threshold_cnt}\")\n",
    "print(f\"threshold pixels ratio: {threshold_cnt / pixel_channel}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '../')\n",
    "from utils.huffman_encoding import (get_freq_dict,\n",
    "                                    build_huffman_tree,\n",
    "                                    generate_huffman_codes)\n",
    "freq = get_freq_dict(diff)\n",
    "root = build_huffman_tree(freq)\n",
    "huffmanCode = generate_huffman_codes(root)\n",
    "\n",
    "sorted_freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "encoded_bits = 0\n",
    "for (char, frequency) in sorted_freq:\n",
    "    # print(f\"Character: {char:>3}, Code: {huffmanCode[char]:>17}, Length of Code: {len(huffmanCode[char]):>2}, Frequency: {frequency:>5}\")\n",
    "    encoded_bits += (len(huffmanCode[char]) * frequency)\n",
    "dict_bits = (len(freq) * 2) * 4 * 8\n",
    "total_bits = encoded_bits + dict_bits\n",
    "print(f\"{len(freq) = }\")\n",
    "print(f\"{total_bits = }\")\n",
    "print(f\"In Bytes  = {total_bits / 8}\")\n",
    "print(f\"In KB     = {total_bits / 8 / 1024}\")\n",
    "print(f\"In MB     = {total_bits / 8 / 1024 / 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.decoder.state_dict(), \"../checkpoints/test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fpzip\n",
    "import os\n",
    "\n",
    "data = np.array(feature, dtype=np.float32) # up to 4d float or double array\n",
    "print(data.shape)\n",
    "# Compress data losslessly, interpreting the underlying buffer in C (default) or F order.\n",
    "compressed_bytes = fpzip.compress(data, precision=0, order='C') # returns byte string\n",
    "with open(\"test.fpzip\", \"wb\") as f:\n",
    "    f.write(compressed_bytes)\n",
    "\n",
    "fpsize = os.path.getsize(\"test.fpzip\")\n",
    "print(f\"fpsize: {fpsize / 1024 / 1024} MB\")\n",
    "\n",
    "with open(\"test.fpzip\", \"rb\") as f:\n",
    "    compressed_bytes = f.read()\n",
    "# Back to 3d or 4d float or double array, decode as C (default) or F order.\n",
    "data_again = fpzip.decompress(compressed_bytes, order='C') \n",
    "print(data_again.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
